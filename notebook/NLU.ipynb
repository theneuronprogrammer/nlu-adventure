{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The following definitions were created using https://talktotransformer.com/ with the mentioned prompts. Check out yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt - \"Natural Language processing is\"\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Prompt 1:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language processing is perhaps the most mysterious part of AI, especially given how easy it is to model natural language information in computer systems. Even using a simple neural network (a math-based model) to run a single sentence, an agent is still too complex to discern with any confidence what it is intended to say.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Prompt 2:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language processing is a branch of cognitive science which explores the way our brains deal with language. Similar to other cognitive science projects, Chomsky's method entails studying natural languages which have been learned by humans and put into a digital format. By simply applying a machine learning algorithm to the database of speech data, a robot can learn to speak naturally and as quickly as a real person.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Prompt 3:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Natural Language processing is divided into the following areas:\n",
    "\n",
    "Generating artificial English language\n",
    "\n",
    "Syntactic analysis\n",
    "\n",
    "Semantic analysis\n",
    "\n",
    "Query handling\n",
    "\n",
    "Supporting natural language queries with shell programs\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Prompt 4:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language processing is the study of information gathered by humans from and through text, images, sounds, and other forms of communication. It also includes parsing and interpreting of that information and its associated grammatical structures and rules. These challenges are often represented as specific tasks for artificial intelligence—such as computer vision, speech recognition, natural language understanding, or the detection of sentiment or emotional bias in written messages—where natural language processing is performed on a text-based language or a non-text. Although these NLP challenges may be more interesting to researchers in that area, they are sometimes conflated with domain-specific task problems such as the implementation and analysis of machine learning models.\n",
    "\n",
    "Natural language processing is fundamentally concerned with the study of data and with the transformation of that data into knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt - \"Natural Language Understanding is\"\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Prompt 1:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language Understanding is a unique branch of AI that is coming up to join the growing array of machine learning algorithms that are helping to process and share big data sets. Currently, there is an interest in this field of AI, particularly among automated decision makers who handle financial and other kind of data, for simple but very interesting tasks such as corporate logo design.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Prompt 2:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language Understanding is the ability to have fast, accurate and rational response to information. Information is not just vague, it must be structured.\n",
    "\n",
    "A good computer will do an 'NLP' to take a sentence of text and distinguish it from that of others. This will be quicker than reading a word-for-word.\n",
    "\n",
    "Language is an energy-flow-based phenomenon. Our energy flows in a way that is consistent with being a part of reality. So this would cause us to find that the same rules apply, regardless of language we speak. It is a behavior.\n",
    "\n",
    "It is common for language to change, based on how the speaker feels, experience, situations, and places.\n",
    "\n",
    "Also Language is a social phenomenon. It is less of a\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Prompt 3:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language Understanding is the ability to learn and master several different languages and to comprehend, use, and understand sentences in those languages. \"Parallel cognition\" refers to the ability to learn multiple systems of language in one being.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "#### Prompt 4:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Natural Language Understanding is a cognitive ability which allows an organism to rapidly and automatically process certain large numbers of pieces of information, the so called ciphers, that are much more complex than any verbal language\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt: \"Natural Language Generation is\"\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "#### Prompt 1:\n",
    "<br><br>\n",
    "Natural Language Generation is a specification for a general-purpose and generic grammar-based natural language generative system. The goal is to be able to generate sentences that are a subset of natural languages as far as accuracy, meaning, and expressive power are concerned. The system must be sensitive to syntax (allowing the programmer to specify how certain symbols must be entered) and semantics (be capable of generating sentences according to rules specified by the user). The main goal of the system is not to model languages, but to be easy to use, general-purpose, and extensible.\n",
    "\n",
    "The design of the Linguistics component is based on previous work done on NLG grammars.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Prompt 2:\n",
    "<br><br>\n",
    "Natural Language Generation is incredibly difficult — mathematicians have been unable to solve it in the past, and good human intelligence is probably still 100,000 years away from that point. However, with parallel computing, the engineering constraints that render AI intelligence challenging for machine learning algorithms are being suddenly relaxed.\n",
    "\n",
    "Predicting the future is dangerous business\n",
    "\n",
    "AI will be autonomous, but it will also have control over every aspect of a system that it creates. This could lead to dangerous situations, such as humans starting wars, or some sort of scenario where an AI acts as judge, jury and executioner of humans. To ensure that AI remains undefended, we need to give it clear directives. Given the wide variety of legal systems (and vague guidelines), some AI will be programmed\n",
    "\n",
    "<br><br>\n",
    "#### Prompt 3:\n",
    "<br><br>\n",
    "\n",
    "Natural Language Generation is a skill that all Coders should have and I do not agree with all these discussions of code smell and more. Nevertheless, it is very easy to convince yourself that code smells can be caught, but it will be much more difficult to convince anyone else.\n",
    "\n",
    "The first thing that you have to do is to ask people about code smells. Every problem on your team is going to have a couple of different smells, however, it is not important which one you catch. Instead, it is better to try to pinpoint a signature smell on your code that solves a specific problem.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers (HuggingFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The authors of HuggingFace have developed Transformers, a library for state-of-the art Natural Language Processing with Transfer Learning models. Transformers addresses several key challenges:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "##### Sharing is caring: \n",
    "<br>\n",
    "\n",
    "    Transformers gathers, in a single place, state-of-the art architectures for both Natural Language Understanding (NLU) and Natural Language Generation (NLG) with model code and a diversity of pretrained weights. This allows a form of training-computation-cost-sharing so that low-resource users can reuse pretrained models without having to train them from scratch. These models are accessed through a simple and uniﬁed API that follows a classic NLP pipeline: setting up conﬁguration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations’ performances on various benchmarks.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "##### Easy-access and high-performance:\n",
    "<br>\n",
    "\n",
    "    Transformers was designed with two main goals in mind: (i) be as easy and fast to use as possible and (ii) provide state-of-the-art models with performances as close as possible to the originally reported results.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Interpretability and diversity:\n",
    "<br>\n",
    "      \n",
    "\n",
    "    There is a growing ﬁeld of study, sometimes referred as BERTology from BERT (Devlin et al., 2018), concerned with investigating the inner working of large-scale pretrained models and trying to build a science on top of these empirical results.\n",
    "    Transformers aims at facilitating and increasing the scope of these studies by (i) giving easy access to the inner representations of these models, notably the hidden states, the attention weights or heads importance as deﬁned in Michel et al. (2019) and (ii) providing diﬀerent models in a uniﬁed API to prevent overﬁtting to a speciﬁc architecture (and set of pretrained weights). Moreover, the uniﬁed front-end of the library makes it easy to compare the performances of several architectures on a common language understanding benchmark. Transformers notably includes pre-processors and ﬁne-tuning scripts for GLUE (Wang et al., 2018), SuperGLUE (Wang et al. (2019)) and SQuAD1.1 (Rajpurkar et al., 2016).\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Pushing best practices forward:\n",
    "<br>\n",
    "\n",
    "    Transformers seeks a balance between sticking to the original authors’ code-base for reliability and providing clear and readable implementations featuring best practices in training deep neural networks so that researchers can seamlessly use the code-base to explore new hypothesis derived from these models. To accommodate a large community of practitioners and researchers, the library is deeply compatible with (and actually makes compatible) two major deep learning frameworks: PyTorch (Paszke et al., 2017) and TensorFlow (from release 2.0) (Abadi et al., 2015). \n",
    "<br><br>\n",
    "##### From research to production:\n",
    "<br>\n",
    "    \n",
    "    Another essential question is how to make these advances in research available to a wider audience, especially in the industry. Transformers also takes steps towards a smoother transition from research to production. The provided models support TorchScript, a way to create serializable and optimizable models from PyTorch code, and features production code and integration with the TensorFlow Extended framework.\n",
    "    \n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Design of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>   \n",
    "    \n",
    "    Transformers has been designed around a uniﬁed frontend for all the models: parameters and conﬁgurations, tokenization, and model inference. These steps reﬂect the recurring questions that arise when building an NLP pipeline: deﬁning the model architecture, processing the text data and ﬁnally, training the model and performing inference in production. In the following section, we’ll give an overview of the three base components of the library: conﬁguration, model and tokenization classes. All of the components are compatible with PyTorch and TensorFlow.\n",
    "    \n",
    "    The documentation is available on : https://huggingface.co/transformers/\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "    Conﬁguration - A conﬁguration class instance (usually inheriting from a base class ‘Pre-trainedConﬁg‘) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This conﬁguration object can be saved and loaded for reproducibility or simply modiﬁed for architecture search.\n",
    "<br><br>\n",
    "\n",
    "    Tokenizers - A Tokenizer class (inheriting from a base class ‘PreTrainedTokenizer‘) is available for each model. This class stores the vocabulary token-to-index map for the corresponding model and handles the encoding and decoding of input sequences according to the model’s tokenization-speciﬁc process.\n",
    "    Furthermore, Tokenizers implement additional useful features for the users, by oﬀering values to be used with a model; these range from token type indices in the case of sequence classiﬁcation to maximum length sequence truncating taking into account the added model-speciﬁc special tokens\n",
    "<br><br>\n",
    "\n",
    "\n",
    "    Model - All models follow the same hierarchy of abstraction: a base class implements the model’s computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is speciﬁc to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a speciﬁc head on top of the base model hidden states.\n",
    "    All models are available both in PyTorch and TensorFlow (starting 2.0) and oﬀer deep inter-operability between both frameworks. For instance, a model trained in one of frameworks can be saved on drive for the standard library serialization practice and then be reloaded from the saved ﬁles in the other framework seamlessly, making it particularly easy to switch from one framework to the other one along the model life-time (training, serving, etc.)\n",
    "<br><br>\n",
    "\n",
    "    Auto classes -- In many cases, the architecture to use can be automatically guessed from the shortcut name of the pretrained weights (e.g. ‘bert-base-cased‘). A set of Auto classes provides a uniﬁed API that enable very fast switching between diﬀerent models/conﬁgs/tokenizers. There are a total of four high-level abstractions referenced as Auto classes: AutoConfig, AutoTokenizer, AutoModel (for PyTorch) and TFAutoModel (for TensorFlow). These classes automatically instantiate the right conﬁguration, tokenizer or model class instance from the name of the pretrained checkpoints.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Optimizer - The library provides a few optimization utilities as subclasses of PyTorch\n",
    "‘torch.optim.Optimizer‘ which can be used when training the models. The additional optimizer\n",
    "currently available is the Adam optimizer (Kingma and Ba, 2014) with an additional weight\n",
    "decay ﬁx, also known as ‘AdamW‘ (Loshchilov and Hutter, 2017).\n",
    "<br>\n",
    "<br>\n",
    "Scheduler - Additional learning rate schedulers are also provided as subclasses of PyTorch\n",
    "‘torch.optim.lr_scheduler.LambdaLR‘, oﬀering various schedules used for transfer learning\n",
    "and transformers models with customizable options including warmup schedules which are\n",
    "relevant when training with Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here is a list of architectures for which reference implementations and pretrained weights\n",
    "are currently provided in Transformers. These models fall into two main categories:\n",
    "generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language\n",
    "understanding (Bert, DistilBert, RoBERTa, XLM)\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### BERT\n",
    "<br>\n",
    "    BERT (Devlin et al. (2018)) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives.\n",
    "    \n",
    "    \n",
    "<br><br>\n",
    "#### RoBERTa\n",
    "<br>\n",
    "    \n",
    "    RoBERTa (Liu et al. (2019)) is a replication study of BERT which showed that care-fully tuning hyper-parameters and training data size lead to significantly improved results on language understanding.\n",
    "<br><br>\n",
    "\n",
    "#### DistilBERT\n",
    "<br>\n",
    "   \n",
    "    DistilBERT(Sanh et al. (2019)) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### GPT\n",
    "<br>\n",
    "\n",
    "    GPT (Radford et al. (2018)) and GPT2 (Radford et al. (2019)) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension.\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "#### Transformer-XL (Dai et al. (2019))\n",
    "\n",
    "    \n",
    "    Introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes.\n",
    "    \n",
    "<br><br>\n",
    "    \n",
    "#### XLNet (Yang et al. (2019))\n",
    "<br>\n",
    "\n",
    "    Builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT’s bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence.\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "\n",
    "#### XLM (Lample and Conneau (2019)) \n",
    "<br>\n",
    "    \n",
    "    Shows the effectiveness of pretrained represen-tations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding.\n",
    "    \n",
    " <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT : Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "    BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    "    \n",
    "<br>\n",
    "\n",
    "    BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re-sult, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "<br>\n",
    "    \n",
    "    Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference, paraphrasing, which aim to predict the re-lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "    There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning. Thefeature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as addi-tional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "    The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "    BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked language model”. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer.\n",
    "    \n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "##### BERT\n",
    "<br>\n",
    "\n",
    "    There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.\n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "##### Model Architecture\n",
    "<br>\n",
    "\n",
    "    BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation.\n",
    "<br>\n",
    "    \n",
    "    BERT BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M). The BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.\n",
    "    \n",
    "<br>\n",
    "\n",
    "##### Input/Output Representations:\n",
    "\n",
    "<br>\n",
    "\n",
    "    To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answer i) in one token sequence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. The first token of every sequence is always a special classification token ([CLS]). Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]).\n",
    "    Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.\n",
    "    \n",
    "    \n",
    "    For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n",
    "    \n",
    "<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We pre-train BERT using two unsupervised tasks.\n",
    "\n",
    "<br>\n",
    "#####  Task #1: Masked LM:\n",
    "\n",
    "<br>\n",
    "    \n",
    "    Since bidirec-tional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\n",
    "    \n",
    "    In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM).\n",
    "    \n",
    "    Although this allows us to obtain a bidirec-tional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction.\n",
    "<br><br>\n",
    "\n",
    "##### Task #2: Next Sentence Prediction (NSP)\n",
    "<br>\n",
    "\n",
    "    Many important downstream tasks such as Ques-tion Answering (QA) and Natural Language Infer-ence (NLI) are based on understanding the rela-tionship between two sentences, which is not di-rectly captured by language modeling. In orderto train a model that understands sentence rela-tionships, we pre-train for a binarized next sen-tence prediction task that can be trivially gener-ated from any monolingual corpus. Specifically,when choosing the sentences A and B for each pre-training example, 50% of the time B is the actualnext sentence that follows A (labeled as IsNext ),and 50% of the time it is a random sentence fromthe corpus (labeled as NotNext ).\n",
    "    \n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pretraining_BERT.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('''<img src=\"pretraining_BERT.png\">'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning BERT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "        Fine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks—whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. \n",
    "    \n",
    "        BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.\n",
    "    \n",
    "        At the in-put, sentence A and sentence B from pre-trainingare analogous to (1) sentence pairs in paraphras-ing, (2) hypothesis-premise pairs in entailment, (3)question-passage pairs in question answering, and (4) a degenerate text-∅ pair in text classificationor sequence tagging. At the output, the token rep-resentations are fed into an output layer for token-level tasks, such as sequence tagging or questionanswering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.\n",
    "    \n",
    "       Compared to pre-training, fine-tuning is relatively inexpensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Pre-training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The importance of the deep bidirectionality of BERT by evaluating two pre-training objectives.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "##### No NSP:\n",
    "<br>\n",
    "A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "##### LTR & No NSP: \n",
    "<br>\n",
    "\n",
    "A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. Additionally, this model was pre-trained without the NSP task. Removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Model Size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n",
    "Larger models lead to a strict accuracy improvement across all four datasets.\n",
    "<br>\n",
    "\n",
    "\n",
    "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling. Scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.\n",
    "<br>\n",
    "\n",
    "When the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-based Approach with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added.Second, there are major computational benefits to pre-compute an expensive representation of the\n",
    "training data once and then run many experiments with cheaper models on top of this representation.\n",
    "\n",
    "<br>\n",
    "\n",
    "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"BERT_OPENGPT_ELMO.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('''<img src=\"BERT_OPENGPT_ELMO.png\">'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMs to generate features for downstream tasks. Among the three, only BERT  representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"bert_different_tasks.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('''<img src=\"bert_different_tasks.png\">'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Number of Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Does BERT really need sucha large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?\n",
    "\n",
    "##### Answer:\n",
    "Yes, BERT BASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "##### Question:\n",
    "Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word?\n",
    "<br>\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
